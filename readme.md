# Othello-Qwen: An Experiment in Teaching Board Game Rules to Small Language Models

*Last Updated: October 10, 2025*

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An experimental project focused on teaching the complex rules of Othello (Reversi) to a small language model (Qwen3-4B) using a multi-task Chain-of-Thought (CoT) fine-tuning approach. The primary goal is not just to create a strong player, but to develop a model that can **internally reason about the game's rules**, making its "thinking" process transparent and robust.

---

### ðŸŽ¯ Project Goal

The central challenge of this project is to bridge the gap between the symbolic, text-based nature of Language Models and the spatial, logical nature of board games. Can a small, efficient LLM be trained to **inductively learn** the rigid rules of Othello, such as identifying legal moves based on the "flanking" rule, without being explicitly programmed for it?

This repository documents an end-to-end pipeline for:
1.  Generating high-quality, rule-based Chain-of-Thought (CoT) data.
2.  Fine-tuning a small language model using Parameter-Efficient Fine-Tuning (PEFT/QLoRA).
3.  Evaluating the model's rule-learning capabilities with rigorous, quantitative metrics.
4.  Benchmarking the fine-tuned specialist model against a powerful, general-purpose LLM.

---

### âœ¨ Key Features

- **Multi-Task Decomposition**: Breaks down the complex task of "making a move" into a logical chain of simpler, learnable sub-tasks.
- **Hybrid CoT Generation**: Utilizes a cost-effective and highly accurate hybrid approach:
  - **Rule-Based CoT**: Deterministic, perfectly accurate CoT data for rule-based tasks is generated using Python code.
  - **LLM-Based CoT**: (Planned for Task 3) A powerful teacher model is used for complex, fuzzy strategic reasoning.
- **High Precision on Rule Following**: The fine-tuned model demonstrates a superior ability to avoid making illegal moves compared to a much larger baseline model.
---

### ðŸ§  Methodology & Architecture

To teach the model the complex logic of Othello, we decompose the decision-making process into a sequence of distinct, manageable tasks. The model is trained on a unified dataset containing examples from each task, learning to perform the correct action based on the specific instruction in the prompt.

The training data for the rule-based tasks (Task 1 and Task 2) is generated programmatically by a deterministic game engine, ensuring 100% accuracy and consistency.

#### Task 1: Plausible Candidate Identification

This initial step trains the model to perform a smart "pruning" of the board, focusing only on areas where a move might be possible.

* **Input**: The current board state, represented as a list of black and white piece coordinates (e.g., `{"black_pieces": ["d4"], "white_pieces": ["d5"]}`).
* **Output**: A JSON object containing a detailed analysis of sampled squares and concluding with a list of "plausible" candidates. A plausible candidate is defined as an empty square that is directly adjacent to at least one opponent piece.

#### Task 2: Legality Analysis

This is the core rule-learning task. The model learns to apply the precise flanking rule of Othello to the plausible candidates identified in the previous step.

* **Input**: The current board state AND the list of plausible candidates generated by Task 1.
* **Output**: A JSON object containing a detailed, step-by-step analysis of each plausible candidate, concluding with a final, verified list of all legal moves.

#### Task 3: Strategic Decision (Future Work)

This final step will train the model to move beyond simply following rules to making strategically sound decisions.

* **Input**: The current board state AND the list of legal moves verified by Task 2.
* **Output**: A JSON object containing a strategic analysis that compares the legal options and selects the single best move, based on expert human game data.

---

### ðŸ“Š Results & Analysis

After fine-tuning a Qwen3-4B model on the rule-based data for Tasks 1 & 2, its ability to identify legal moves was evaluated and benchmarked against the powerful `deepseek-v3` model in a zero-shot setting.

**Evaluation Metric:** The evaluation focuses on the model's performance on **Task 2: Legality Analysis** using the plausible candidate it generated before.

| Model               | Precision | Recall | F1-Score |
| ------------------- | :-------: | :----: | :------: |
| **deepseek-v3** (Benchmark) |  0.8850   | 0.6917 |  0.7765  |
| **othello_qwen** (Fine-tuned) | **0.9399** | 0.5758 |  0.7141  |

### Analysis of Results

1.  **Exceptional Precision - The Model Learned the Rules**:
    The most significant result is our fine-tuned `othello_qwen` model's **Precision of ~94%**. This metric indicates that when our model identifies a move as legal, it is correct an overwhelming majority of the time. This success demonstrates that the multi-task, rule-based CoT fine-tuning was highly effective at teaching the model the core flanking rule of Othello. Our small, specialist model **significantly surpassed the large, generalist `deepseek-v3` model in reliability** (94% vs. 88.5%), proving the value of domain-specific fine-tuning.

2.  **Explaining the Low Recall - An Artifact of Context Limitations**:
    At first glance, the model's **Recall of ~58%** appears to be a major weakness. However, this is primarily an **artifact of an engineering constraint**, not a failure of the model's reasoning ability. Due to the model's limited context window (2048 tokens for this experiment), the prompt for Task 2 can only include a maximum of 10 plausible candidates for analysis.

    In game states with more than 10 plausible moves, a random subset is selected. If a true legal move was not included in this sampled subset, the model had **zero opportunity** to identify it. Therefore, the low Recall score is not a reflection of the model "missing" moves it saw, but rather a consequence of the inference pipeline's limitation. The model is, in fact, highly effective at correctly analyzing the candidates it is presented with.

---

### ðŸ“ˆ Future Work

-   [ ] **Improve Recall via Pipeline Enhancements**: Address the context window limitation that currently bottlenecks Recall. 
-   [ ] **Implement and Train for Task 3 (Strategic Reasoning)**: Generate strategic CoT data using a powerful teacher model and train the agent to not only follow rules but also make strategically sound decisions from the list of legal moves.
-   [ ] **Reinforcement Learning**: After the supervised fine-tuning phase, use reinforcement learning to further enhance the agent's playing strength through self-play, moving from pure rule-learning to strategy optimization.
-   [ ] **Explore Different Model Sizes**: Test if larger models (e.g., 7B) with larger context windows can achieve a better balance of precision and recall out-of-the-box.